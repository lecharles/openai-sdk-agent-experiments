# Top Papers

## 1. Gap the (Theory of) Mind: Sharing Beliefs About Teammates’ Goals Boosts Collaboration Perception, Not Performance
- **Authors:** Yotam Amitai, Reuth Mirsky, Ofra Amir
- **Year:** 2025
- **Month:** May
- **Technique Type:** Other
- **Technique Description:** 
- **Summary:** The paper investigates whether an AI agent's ability to share its inferred understanding of a human teammate's goals can improve task performance and perceived collaboration. Through an experiment comparing three conditions—no recognition (NR), viable goals (VG), and viable goals on-demand (VGod)—the study finds that while goal-sharing information did not yield significant improvements in task performance or overall satisfaction scores, it supported strategic adaptations and subjective perceptions of collaboration. The findings highlight the nuanced trade-off of goal-sharing in human-agent collaboration.

## 2. A HASHGRAPH -INSPIRED CONSENSUS MECHANISM FOR RELIABLE MULTI -MODEL REASONING
- **Authors:** Kolawole E. Ogunsina, Morayo A. Ogunsina
- **Year:** 2025
- **Month:** May
- **Technique Type:** Other
- **Technique Description:** 
- **Summary:** This paper proposes a novel consensus mechanism inspired by distributed ledger technology to validate and converge outputs from different proprietary reasoning models. The mechanism, based on the Hashgraph consensus algorithm, employs gossip-about-gossip communication and virtual voting to achieve agreement among an ensemble of models. The approach aims to minimize hallucinations and maximize fidelity in AI systems by ensuring consistency, fairness, and Byzantine fault tolerance. The paper presents an architectural design for a prototype system where models iteratively exchange and update their answers to improve accuracy and confidence in subsequent rounds.

## 3. STORY 2GAME : Generating (Almost) Everything in an Interactive Fiction Game
- **Authors:** Eric Zhou, Shreyas Basavatia, Moontashir Siam, Zexin Chen, Mark O. Riedl
- **Year:** 2025
- **Month:** May
- **Technique Type:** Language Model Technique
- **Technique Description:** Large Language Models are used to generate text-based interactive fiction games by creating stories, populating worlds, and building action codes for gameplay.
- **Summary:** This paper introduces STORY 2GAME, a novel approach using Large Language Models to generate text-based interactive fiction games. The system starts by generating a story, populating the world, and building the code for actions in a game engine to enable interactive gameplay. It addresses the challenge of dynamically generating new actions to accommodate player input not part of the original story.

## 4. am-ELO: A Stable Framework for Arena-based LLM Evaluation
- **Authors:** Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang
- **Year:** 2025
- **Month:** May
- **Technique Type:** Language Model Technique
- **Technique Description:** This paper presents a stable arena framework, am-ELO, for evaluating large language models (LLMs) by enhancing the ELO Rating System with a Maximum Likelihood Estimation (MLE) approach and incorporating annotator abilities into the evaluation process.
- **Summary:** This paper introduces a novel stable arena framework, am-ELO, to address the instability problem in existing arena-based evaluation systems for large language models (LLMs). The framework enhances the ELO Rating System by replacing the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO. Additionally, it modifies the ELO probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experimental results demonstrate that this method ensures stability and offers a more robust, accurate, and stable evaluation method for LLMs.

## 5. Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children’s Development across Various Free Play Settings in Kindergarten
- **Authors:** Yuanyuan Yanga, Yuan Shena, Tianchen Suna, Yangbin Xiea
- **Year:** 2025
- **Month:** May
- **Technique Type:** Language Model Technique
- **Technique Description:** The paper describes the use of Large Language Models (LLMs) combined with learning analytics to analyze children's self-narratives of their play experiences in kindergarten, identifying developmental abilities and calculating performance scores across different play settings.
- **Summary:** This study proposes an innovative approach combining Large Language Models (LLMs) with learning analytics to analyze children’s self-narratives of their play experiences in kindergarten. The LLM identifies developmental abilities, while performance scores across different play settings are calculated using learning analytics techniques. The evaluation results show high accuracy in identifying cognitive, motor, and social abilities, with significant differences in developmental outcomes across play settings.

## 6. AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning
- **Authors:** Evgeny Markhasin
- **Year:** 2021
- **Month:** 
- **Technique Type:** Prompt Engineering Technique
- **Technique Description:** Persistent Workflow Prompting (PWP) methodology designed to guide Large Language Models (LLMs) through detailed analysis workflows by developing structured prompts and incorporating meta-prompting and meta-reasoning techniques.
- **Summary:** This paper introduces Persistent Workflow Prompting (PWP), a prompt engineering methodology designed to facilitate critical peer review of scientific manuscripts using Large Language Models (LLMs). The methodology involves developing structured prompts to guide LLMs through detailed analysis workflows, incorporating meta-prompting and meta-reasoning techniques to codify expert review workflows. The study focuses on AI-driven scholarly peer review, demonstrating the potential of PWP to enable sophisticated analysis using readily available LLMs for complex scientific tasks.

## 7. RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation
- **Authors:** Tiantian Gan, Qiyao Sun
- **Year:** 2022
- **Month:** May
- **Technique Type:** Prompt Engineering Technique
- **Technique Description:** RAG-MCP utilizes semantic retrieval to dynamically select relevant tools for a query, reducing prompt size and decision complexity.
- **Summary:** Large language models (LLMs) struggle with prompt bloat and selection complexity when utilizing external tools. RAG-MCP introduces a framework that uses semantic retrieval to identify relevant tools for a query, reducing prompt size and simplifying decision-making. Experiments show significant improvements in tool selection accuracy and prompt token reduction.

## 8. Holmes: Automated Fact Check with Large Language Models
- **Authors:** Haoran Ou, Gelei Deng, Xingshuo Han, Jie Zhang, Xinlei He, Han Qiu, Shangwei Guo, Tianwei Zhang
- **Year:** 2025
- **Month:** May
- **Technique Type:** Language Model Technique
- **Technique Description:** The paper describes the use of Large Language Models (LLMs) for automated fact-checking and disinformation detection, focusing on verifying multimodal disinformation and proposing a novel evidence retrieval methodology.
- **Summary:** The paper presents Holmes, an end-to-end framework for automated fact-checking and disinformation detection using Large Language Models (LLMs). It addresses the limitations of LLMs in verifying multimodal disinformation by proposing a novel evidence retrieval methodology. Holmes achieves an accuracy of 88.3% in two open-source datasets and 90.2% in a real-time disinformation verification task.

## 9. Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE
- **Authors:** Brendan Campbell, Alan Williams, Kleio Baxevani, Alyssa Campbell, Rushabh Dhoke, Rileigh E. Hudock, Xiaomin Lin, Vivek Mange, Bernhard Neuberger, Arjun Suresh, Alhim Vera, Arthur Trembanis, Herbert G. Tanner, Edward Hale
- **Year:** 2025
- **Month:** May
- **Technique Type:** Other
- **Technique Description:** 
- **Summary:** Oysters are ecologically and commercially important species that require frequent monitoring. The ODYSSEE model, using deep learning techniques, was developed to identify live oysters on reefs. The model's accuracy was compared to expert and non-expert annotators, revealing potential sources of prediction error. Although the model was faster, it overpredicted live oysters and had lower accuracy compared to human annotators. Future improvements include training on higher-quality images and additional annotation classes.

## 10. Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes
- **Authors:** George Wang, Jingying Deng, Safinah Ali
- **Year:** 2025
- **Month:** 
- **Technique Type:** Other
- **Technique Description:** 
- **Summary:** The paper introduces an AI-powered system that uses Large Language Models (LLMs) to generate personalized multisensory study environments aimed at reducing distraction and enhancing emotional stability. It explores how combinations of personalized audiovisual elements affect learners' cognitive load and engagement. The system, called Whisper, allows users to create personalized environments by selecting visual themes and auditory elements. The research evaluates the effectiveness of LLM-driven sensory personalization in improving learning efficiency and emotional states.
